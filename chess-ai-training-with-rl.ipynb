{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T15:14:06.035629Z",
     "iopub.status.busy": "2025-01-13T15:14:06.035357Z",
     "iopub.status.idle": "2025-01-13T15:14:14.457977Z",
     "shell.execute_reply": "2025-01-13T15:14:14.457122Z",
     "shell.execute_reply.started": "2025-01-13T15:14:06.035592Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T15:14:14.459221Z",
     "iopub.status.busy": "2025-01-13T15:14:14.458974Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from ray.rllib.models.torch.misc import SlimFC, normc_initializer\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.env.env_context import EnvContext\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.algorithms.ppo import PPO\n",
    "from datetime import datetime\n",
    "import gymnasium as gym\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import torch\n",
    "import chess\n",
    "import ray\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.version\n",
    "\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "bcf1edce-b42d-4b77-8f4e-ada95dc63b06",
    "_uuid": "dc533249-483f-48d3-aa3a-d78518f168c6",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class RayChessEnvironment(gym.Env):\n",
    "    def __init__(self, config: EnvContext):\n",
    "        super().__init__()\n",
    "        self.board = chess.Board()\n",
    "        self.move_table = self.build_move_table()\n",
    "        self.move_lookup = {move: idx for idx, move in enumerate(self.move_table)}\n",
    "        self.action_space = gym.spaces.Discrete(len(self.move_table))\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0.0, high=1.0, shape=(19, 8, 8), dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def build_move_table(self):\n",
    "        move_table = []\n",
    "        for from_square in range(64):\n",
    "            for to_square in range(64):\n",
    "                if from_square != to_square:\n",
    "                    move_table.append(chess.Move(from_square, to_square))\n",
    "        \n",
    "        promotion_ranks = ((1, 0), (6, 7))\n",
    "        for from_rank, to_rank in promotion_ranks:\n",
    "            for file in range(8):\n",
    "                from_square = from_rank * 8 + file\n",
    "                to_square = to_rank * 8 + file\n",
    "\n",
    "                for promotion in (chess.QUEEN, chess.ROOK, chess.BISHOP, chess.KNIGHT):\n",
    "                    move_table.append(chess.Move(from_square, to_square, promotion))\n",
    "\n",
    "                if file > 0:\n",
    "                    to_square = to_rank * 8 + (file - 1)\n",
    "                    for promotion in (chess.QUEEN, chess.ROOK, chess.BISHOP, chess.KNIGHT):\n",
    "                        move_table.append(chess.Move(from_square, to_square, promotion))\n",
    "\n",
    "                if file < 7:\n",
    "                    to_square = to_rank * 8 + (file + 1)\n",
    "                    for promotion in (chess.QUEEN, chess.ROOK, chess.BISHOP, chess.KNIGHT):\n",
    "                        move_table.append(chess.Move(from_square, to_square, promotion))\n",
    "        \n",
    "        return move_table\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        self.board.reset()\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        return self.get_observation(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        move = self.move_table[action]\n",
    "\n",
    "        try:\n",
    "            if move in self.board.legal_moves:\n",
    "                self.board.push(move)\n",
    "            else:\n",
    "                legal_moves = list(self.board.legal_moves)\n",
    "                if legal_moves:\n",
    "                    self.board.push(np.random.choice(legal_moves))\n",
    "            \n",
    "            obs = self.get_observation()\n",
    "            assert (obs >= 0).all() and (obs <= 1).all(), \"Observation out of bounds\"\n",
    "            \n",
    "            reward = self.get_reward()\n",
    "            done = self.board.is_game_over()\n",
    "            return obs, reward, done, False, {}\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in step: {e}\")\n",
    "            raise\n",
    "\n",
    "    def get_observation(self):\n",
    "        board_tensor = np.zeros((19, 8, 8), dtype=np.float32)\n",
    "        \n",
    "        # Piece planes (0-11)\n",
    "        for square, piece in self.board.piece_map().items():\n",
    "            piece_plane = {\n",
    "                'P': 0, 'N': 1, 'B': 2, 'R': 3, 'Q': 4, 'K': 5,\n",
    "                'p': 6, 'n': 7, 'b': 8, 'r': 9, 'q': 10, 'k': 11\n",
    "            }[piece.symbol()]\n",
    "            rank, file = divmod(square, 8)\n",
    "            board_tensor[piece_plane, rank, file] = 1.0\n",
    "\n",
    "        # State planes (12-18)\n",
    "        state_planes = np.array([\n",
    "            float(self.board.turn),\n",
    "            float(self.board.has_kingside_castling_rights(True)),\n",
    "            float(self.board.has_queenside_castling_rights(True)),\n",
    "            float(self.board.has_kingside_castling_rights(False)),\n",
    "            float(self.board.has_queenside_castling_rights(False)),\n",
    "            float(self.board.is_check()),\n",
    "            min(1.0, self.board.halfmove_clock / 100.0)  # Clip to ensure <= 1.0\n",
    "        ])\n",
    "        \n",
    "        # Broadcast state planes to 8x8\n",
    "        for i, plane in enumerate(state_planes):\n",
    "            board_tensor[12 + i] = np.full((8, 8), plane)\n",
    "        \n",
    "        return board_tensor\n",
    "\n",
    "    def get_reward(self):\n",
    "        if self.board.is_checkmate():\n",
    "            return 1.0 if not self.board.turn else -1.0\n",
    "        return 0.0\n",
    "\n",
    "    @staticmethod\n",
    "    def env_creator(config: EnvContext):\n",
    "        return RayChessEnvironment(config)\n",
    "\n",
    "class ChessModel(TorchModelV2, nn.Module):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs, model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "\n",
    "        # CNN layers for processing the chess board\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(19, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Calculate the flattened size after convolutions\n",
    "        conv_out_size = 256 * 8 * 8  # Since padding preserves spatial dimensions\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            SlimFC(\n",
    "                in_size=conv_out_size,\n",
    "                out_size=1024,\n",
    "                initializer=normc_initializer(1.0),\n",
    "                activation_fn=\"relu\"\n",
    "            ),\n",
    "            SlimFC(\n",
    "                in_size=1024,\n",
    "                out_size=512,\n",
    "                initializer=normc_initializer(1.0),\n",
    "                activation_fn=\"relu\"\n",
    "            ),\n",
    "            SlimFC(\n",
    "                in_size=512,\n",
    "                out_size=num_outputs,\n",
    "                initializer=normc_initializer(0.01),\n",
    "                activation_fn=None\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Value branch\n",
    "        self.value_branch = nn.Sequential(\n",
    "            SlimFC(\n",
    "                in_size=conv_out_size,\n",
    "                out_size=512,\n",
    "                initializer=normc_initializer(1.0),\n",
    "                activation_fn=\"relu\"\n",
    "            ),\n",
    "            SlimFC(\n",
    "                in_size=512,\n",
    "                out_size=1,\n",
    "                initializer=normc_initializer(0.01),\n",
    "                activation_fn=None\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Variable for storing the value output\n",
    "        self._value_out = None\n",
    "\n",
    "    @override(TorchModelV2)\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        # Process observation through CNN\n",
    "        x = input_dict[\"obs\"].float()  # Shape: [B, 19, 8, 8]\n",
    "        conv_out = self.conv_layers(x)\n",
    "        \n",
    "        # Flatten\n",
    "        conv_flat = torch.flatten(conv_out, start_dim=1)\n",
    "        \n",
    "        # Compute action logits\n",
    "        logits = self.fc_layers(conv_flat)\n",
    "        \n",
    "        # Compute value\n",
    "        self._value_out = self.value_branch(conv_flat).squeeze(1)\n",
    "        \n",
    "        return logits, state\n",
    "\n",
    "    @override(TorchModelV2)\n",
    "    def value_function(self):\n",
    "        return self._value_out\n",
    "    \n",
    "def setup_logging():\n",
    "    # Create logs directory if it doesn't exist\n",
    "    log_dir = \"/kaggle/working/chess_training_logs\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    # Create a timestamped log file\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    log_file = os.path.join(log_dir, f\"training_log_{timestamp}.txt\")\n",
    "    \n",
    "    return log_file\n",
    "\n",
    "def save_log(log_file, episode, result):\n",
    "    with open(log_file, \"a\") as f:\n",
    "        f.write(f\"Episode {episode}:\\n\")\n",
    "        metrics = {\n",
    "            \"episode_reward_mean\": result.get(\"episode_reward_mean\"),\n",
    "            \"episode_len_mean\": result.get(\"episode_len_mean\"),\n",
    "            \"training_iteration\": result.get(\"training_iteration\"),\n",
    "            \"timesteps_total\": result.get(\"timesteps_total\"),\n",
    "            \"time_total_s\": result.get(\"time_total_s\"),\n",
    "        }\n",
    "        # Write summarized metrics\n",
    "        for key, value in metrics.items():\n",
    "            f.write(f\"  {key}: {value}\\n\")\n",
    "\n",
    "def main():\n",
    "    # Register the custom environment\n",
    "    register_env(\"RayChessEnv\", RayChessEnvironment.env_creator)\n",
    "\n",
    "    # Define PPO training configuration\n",
    "    config = {\n",
    "        \"env\": \"RayChessEnv\",\n",
    "        \"framework\": \"torch\",\n",
    "        \"num_workers\": 4,\n",
    "        \"num_envs_per_worker\": 16,\n",
    "        \"num_gpus\": 1,\n",
    "        \"env_config\": {},\n",
    "\n",
    "        \"_enable_new_api_stack\": False,\n",
    "\n",
    "        \"model\": {\n",
    "            \"custom_model\": ChessModel,\n",
    "            \"custom_model_config\": {},\n",
    "        },\n",
    "\n",
    "        \"lr\": 1e-4,\n",
    "        \"train_batch_size\": 1024,\n",
    "        \"sgd_minibatch_size\": 128,\n",
    "        \"num_sgd_iter\": 10,\n",
    "        \"gamma\": 0.99,\n",
    "        \"lambda\": 0.95,\n",
    "        \"clip_param\": 0.2,\n",
    "        \"vf_clip_param\": 10.0,\n",
    "        \"entropy_coeff\": 0.01,\n",
    "\n",
    "        \"num_gpus_per_worker\": 0.05,\n",
    "        \"rollout_fragment_length\": 'auto',\n",
    "        \"batch_mode\": \"truncate_episodes\",\n",
    "    }\n",
    "\n",
    "    # Initialize Ray and set up logging\n",
    "    ray.init()\n",
    "    log_file = setup_logging()\n",
    "    log_dir = os.path.dirname(log_file)  # Directory for logs and checkpoints\n",
    "\n",
    "    # Create PPO trainer\n",
    "    trainer = PPO(config=config)\n",
    "\n",
    "    # Training loop with reduced logging\n",
    "    for episode in range(1024):\n",
    "        result = trainer.train()\n",
    "\n",
    "        # Log every 10 episodes\n",
    "        if episode % 10 == 0:\n",
    "            save_log(log_file, episode, result)\n",
    "            print(f\"Episode {episode}: {result.get('episode_reward_mean')} mean reward\")\n",
    "\n",
    "        # Save checkpoints every 100 episodes\n",
    "        if episode % 100 == 0:\n",
    "            checkpoint = trainer.save(log_dir)\n",
    "            print(f\"Checkpoint saved at: {checkpoint}\")\n",
    "            with open(log_file, \"a\") as f:\n",
    "                f.write(f\"\\nCheckpoint saved at: {checkpoint}\\n\")\n",
    "\n",
    "    # Cleanup\n",
    "    ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=67688, ip=127.0.0.1, actor_id=de374220ac91a9d41733e15401000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000001C32837FF50>)\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1879, in ray._raylet.execute_task\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1820, in ray._raylet.execute_task.function_executor\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m   File \"c:\\Users\\yasse\\chess\\.conda\\Lib\\site-packages\\ray\\_private\\function_manager.py\", line 696, in actor_method_executor\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m   File \"c:\\Users\\yasse\\chess\\.conda\\Lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 467, in _resume_span\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m   File \"c:\\Users\\yasse\\chess\\.conda\\Lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 517, in __init__\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m     self._update_policy_map(policy_dict=self.policy_dict)\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m   File \"c:\\Users\\yasse\\chess\\.conda\\Lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 467, in _resume_span\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m   File \"c:\\Users\\yasse\\chess\\.conda\\Lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1726, in _update_policy_map\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m     self._build_policy_map(\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m   File \"c:\\Users\\yasse\\chess\\.conda\\Lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 467, in _resume_span\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m   File \"c:\\Users\\yasse\\chess\\.conda\\Lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1830, in _build_policy_map\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m     new_policy = create_policy_for_framework(\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m   File \"c:\\Users\\yasse\\chess\\.conda\\Lib\\site-packages\\ray\\rllib\\utils\\policy.py\", line 108, in create_policy_for_framework\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m     return policy_class(observation_space, action_space, merged_config)\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m   File \"c:\\Users\\yasse\\chess\\.conda\\Lib\\site-packages\\ray\\rllib\\algorithms\\ppo\\ppo_torch_policy.py\", line 49, in __init__\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m     TorchPolicyV2.__init__(\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m   File \"c:\\Users\\yasse\\chess\\.conda\\Lib\\site-packages\\ray\\rllib\\policy\\torch_policy_v2.py\", line 176, in __init__\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m     self._optimizers = force_list(self.optimizer())\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m                                   ^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m   File \"c:\\Users\\yasse\\chess\\.conda\\Lib\\site-packages\\ray\\rllib\\policy\\torch_policy_v2.py\", line 424, in optimizer\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m     torch.optim.Adam(self.model.parameters(), lr=self.config[\"lr\"])\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m   File \"c:\\Users\\yasse\\chess\\.conda\\Lib\\site-packages\\torch\\optim\\adam.py\", line 73, in __init__\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m     super().__init__(params, defaults)\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m   File \"c:\\Users\\yasse\\chess\\.conda\\Lib\\site-packages\\torch\\optim\\optimizer.py\", line 367, in __init__\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m     self.add_param_group(cast(dict, param_group))\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m   File \"c:\\Users\\yasse\\chess\\.conda\\Lib\\site-packages\\torch\\_compile.py\", line 26, in inner\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m     import torch._dynamo\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m   File \"c:\\Users\\yasse\\chess\\.conda\\Lib\\site-packages\\torch\\_dynamo\\__init__.py\", line 2, in <module>\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m     from . import convert_frame, eval_frame, resume_execution\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m   File \"c:\\Users\\yasse\\chess\\.conda\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 48, in <module>\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m     from . import config, exc, trace_rules\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m   File \"c:\\Users\\yasse\\chess\\.conda\\Lib\\site-packages\\torch\\_dynamo\\trace_rules.py\", line 52, in <module>\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m     from .variables import (\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m   File \"c:\\Users\\yasse\\chess\\.conda\\Lib\\site-packages\\torch\\_dynamo\\variables\\__init__.py\", line 38, in <module>\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m     from .higher_order_ops import (\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m   File \"c:\\Users\\yasse\\chess\\.conda\\Lib\\site-packages\\torch\\_dynamo\\variables\\higher_order_ops.py\", line 14, in <module>\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m     import torch.onnx.operators\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m   File \"c:\\Users\\yasse\\chess\\.conda\\Lib\\site-packages\\torch\\onnx\\__init__.py\", line 49, in <module>\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m     from ._internal.exporter import (  # usort:skip. needs to be last to avoid circular import\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m   File \"c:\\Users\\yasse\\chess\\.conda\\Lib\\site-packages\\torch\\onnx\\_internal\\exporter\\__init__.py\", line 13, in <module>\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m     from ._analysis import analyze\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m   File \"c:\\Users\\yasse\\chess\\.conda\\Lib\\site-packages\\torch\\onnx\\_internal\\exporter\\_analysis.py\", line 14, in <module>\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m     import torch._export.serde.schema\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m   File \"c:\\Users\\yasse\\chess\\.conda\\Lib\\site-packages\\torch\\_export\\__init__.py\", line 33, in <module>\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m     from torch._export.non_strict_utils import make_constraints\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m   File \"c:\\Users\\yasse\\chess\\.conda\\Lib\\site-packages\\torch\\_export\\non_strict_utils.py\", line 16, in <module>\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m     from torch._dynamo.variables.builder import TrackedFake\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m   File \"c:\\Users\\yasse\\chess\\.conda\\Lib\\site-packages\\torch\\_dynamo\\variables\\builder.py\", line 73, in <module>\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m     from ..trace_rules import (\n",
      "\u001b[36m(RolloutWorker pid=67688)\u001b[0m ImportError: cannot import name 'is_callable_allowed' from partially initialized module 'torch._dynamo.trace_rules' (most likely due to a circular import) (c:\\Users\\yasse\\chess\\.conda\\Lib\\site-packages\\torch\\_dynamo\\trace_rules.py)\n",
      "\u001b[36m(RolloutWorker pid=101928)\u001b[0m     TorchPolicyV2.__init__(\n",
      "\u001b[36m(RolloutWorker pid=90232)\u001b[0m     TorchPolicyV2.__init__(\n",
      "\u001b[36m(RolloutWorker pid=90852)\u001b[0m     TorchPolicyV2.__init__(\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 9818394,
     "sourceId": 86524,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
